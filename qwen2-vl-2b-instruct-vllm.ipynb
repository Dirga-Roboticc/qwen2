{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3606195-8ebd-4856-8343-4868a2daa407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-14 08:39:05 llm_engine.py:232] Initializing an LLM engine (v0.6.1) with config: model='Qwen/Qwen2-VL-2B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2-VL-2B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2-VL-2B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "INFO 10-14 08:39:07 model_runner.py:997] Starting to load model Qwen/Qwen2-VL-2B-Instruct...\n",
      "INFO 10-14 08:39:07 weight_utils.py:242] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230a5e70556f4421b437d955c0801869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-14 08:39:10 model_runner.py:1008] Loading model weights took 4.3411 GB\n",
      "INFO 10-14 08:39:18 gpu_executor.py:122] # GPU blocks: 22995, # CPU blocks: 9362\n",
      "INFO 10-14 08:39:23 model_runner.py:1309] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-14 08:39:23 model_runner.py:1313] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-14 08:39:40 model_runner.py:1428] Graph capturing finished in 17 secs.\n"
     ]
    }
   ],
   "source": [
    "# !pip install git+https://github.com/huggingface/transformers vllm==0.6.1\n",
    "# !pip install git+https://github.com/huggingface/transformers@21fac7abba2a37fae86106f87fcf9974fd1e3830 accelerate\n",
    "# !pip install qwen-vl-utils[decord]\n",
    "from transformers import AutoProcessor\n",
    "from vllm import LLM, SamplingParams\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "MODEL_PATH = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "llm = LLM(\n",
    "    model=MODEL_PATH,\n",
    "    limit_mm_per_prompt={\"image\": 10, \"video\": 10},\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.1,\n",
    "    top_p=0.001,\n",
    "    repetition_penalty=1.05,\n",
    "    max_tokens=256,\n",
    "    stop_token_ids=[],\n",
    "    skip_special_tokens=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0993b85e-b799-4f8e-8ddf-5314537b7ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.83s/it, est. speed input: 1377.94 toks/s, output: 21.31 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "<|object_ref_start|>search icon<|object_ref_end|><|box_start|>(498,10),(532,37)<|box_end|> of the left sidebar of visual studio code and Generate the caption in English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"./tes.png\",\n",
    "                # \"min_pixels\": 224 * 224,\n",
    "                # \"max_pixels\": 1280 * 28 * 28,\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"find search icon of the left sidebar of visual studio code and Generate the caption in English with grounding:\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "# For video input, you can pass following values instead:\n",
    "# \"type\": \"video\",\n",
    "# \"video\": \"<video URL>\",\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_PATH)\n",
    "prompt = processor.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "mm_data = {}\n",
    "if image_inputs is not None:\n",
    "    mm_data[\"image\"] = image_inputs\n",
    "if video_inputs is not None:\n",
    "    mm_data[\"video\"] = video_inputs\n",
    "\n",
    "llm_inputs = {\n",
    "    \"prompt\": prompt,\n",
    "    \"multi_modal_data\": mm_data,\n",
    "}\n",
    "\n",
    "outputs = llm.generate([llm_inputs], sampling_params=sampling_params)\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a243258-24be-4016-a246-853c2399ea00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
